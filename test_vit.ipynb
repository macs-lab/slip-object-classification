{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import glob\n",
    "from itertools import chain\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from linformer import Linformer\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from vit_pytorch.efficient import ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.3.1+cu121\n"
     ]
    }
   ],
   "source": [
    "print(f\"Torch: {torch.__version__}\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "lr = 3e-5\n",
    "gamma = 0.7\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = './train'\n",
    "\n",
    "train_list = []\n",
    "for subdir, dirs, files in os.walk(root_dir):\n",
    "    if os.path.basename(subdir) == 'imgs':\n",
    "        for file in files:\n",
    "            train_list.append(os.path.join(subdir, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27773\n",
      "1271\n"
     ]
    }
   ],
   "source": [
    "print(len(train_list))\n",
    "random.shuffle(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27773\n"
     ]
    }
   ],
   "source": [
    "labels = [path.split('/')[-1].split('.')[2] for path in train_list]\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "groups = [path.split('/')[-1].split('.')[0] for path in train_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list_og = train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_idx = np.random.randint(1, len(train_list), size=9)\n",
    "# # random_idx = [500, 600, 700, 800, 900, 1000, 2000, 1100, 1200]\n",
    "# fig, axes = plt.subplots(3, 3, figsize=(16, 12))\n",
    "\n",
    "# for idx, ax in enumerate(axes.ravel()):\n",
    "#     img = Image.open(train_list[idx])\n",
    "#     ax.set_title(labels[idx])\n",
    "#     ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list, valid_list = train_test_split(train_list, \n",
    "                                          test_size=0.2,\n",
    "                                          stratify=labels,\n",
    "                                          random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data: 22218\n",
      "Validation Data: 5555\n",
      "Test Data: 1271\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train Data: {len(train_list)}\")\n",
    "print(f\"Validation Data: {len(valid_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectClassification(Dataset):\n",
    "    def __init__(self, file_list, transform=None):\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        self.filelength = len(self.file_list)\n",
    "        return self.filelength\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.file_list[idx]\n",
    "        img = Image.open(img_path)\n",
    "        img_transformed = self.transform(img)\n",
    "\n",
    "        label = img_path.split(\"/\")[-1].split(\".\")[2]\n",
    "        # label = 'hard'\n",
    "        label = 1 if label == \"hard\" else 0\n",
    "\n",
    "        return img_transformed, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ObjectClassification(train_list, transform=train_transforms)\n",
    "valid_data = ObjectClassification(valid_list, transform=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = train_data, batch_size=batch_size, shuffle=True )\n",
    "valid_loader = DataLoader(dataset = valid_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22218 695\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data), len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5555 174\n"
     ]
    }
   ],
   "source": [
    "print(len(valid_data), len(valid_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficient_transformer = Linformer(\n",
    "    dim=128,\n",
    "    seq_len=49+1,  # 7x7 patches + 1 cls-token\n",
    "    depth=12,\n",
    "    heads=8,\n",
    "    k=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_pytorch.vit_for_small_dataset import ViT\n",
    "\n",
    "model = ViT(\n",
    "    image_size = 224,\n",
    "    patch_size = 32,\n",
    "    num_classes = 2,\n",
    "    dim = 128,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 2048,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "# scheduler\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d6bb961542443a8a13845986fdc20d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/695 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 - loss : 0.5652 - acc: 0.6689 - val_loss : 0.4453 - val_acc: 0.7641\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cce958283b24d3fac2b5e8ba252b182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/695 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2 - loss : 0.4813 - acc: 0.7508 - val_loss : 0.3627 - val_acc: 0.8682\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4821f07799f44d5d8c191b23ceb31985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/695 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3 - loss : 0.4520 - acc: 0.7732 - val_loss : 0.4493 - val_acc: 0.7696\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66c649c990154ffd97242e223eb5e7f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/695 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 4 - loss : 0.4352 - acc: 0.7882 - val_loss : 0.3667 - val_acc: 0.8497\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd539955c3bb4ab2a4a91de79c9cc02f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/695 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 5 - loss : 0.4030 - acc: 0.8088 - val_loss : 0.2799 - val_acc: 0.8914\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85660fc768bd4ef89c95a04740cfd656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/695 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 6 - loss : 0.3771 - acc: 0.8255 - val_loss : 0.4337 - val_acc: 0.7521\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa7dd45b3484562858e6e68adb64c27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/695 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 7 - loss : 0.3565 - acc: 0.8375 - val_loss : 0.2561 - val_acc: 0.8948\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c51eeb3b15b4aeca97d6d711167f761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/695 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 8 - loss : 0.3414 - acc: 0.8447 - val_loss : 0.2512 - val_acc: 0.9066\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97a6cbe045f4c9dbceea051d9ccac80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/695 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 9 - loss : 0.3197 - acc: 0.8579 - val_loss : 0.2058 - val_acc: 0.9169\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d115c786979040e48a3bb1f01b33fe07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/695 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 10 - loss : 0.3038 - acc: 0.8642 - val_loss : 0.2019 - val_acc: 0.9241\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "\n",
    "    for data, label in tqdm(train_loader):\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        acc = (output.argmax(dim=1) == label).float().mean()\n",
    "        epoch_accuracy += acc / len(train_loader)\n",
    "        epoch_loss += loss / len(train_loader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        epoch_val_accuracy = 0\n",
    "        epoch_val_loss = 0\n",
    "        for data, label in valid_loader:\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            val_output = model(data)\n",
    "            val_loss = criterion(val_output, label)\n",
    "\n",
    "            acc = (val_output.argmax(dim=1) == label).float().mean()\n",
    "            epoch_val_accuracy += acc / len(valid_loader)\n",
    "            epoch_val_loss += val_loss / len(valid_loader)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy:.4f} - val_loss : {epoch_val_loss:.4f} - val_acc: {epoch_val_accuracy:.4f}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './trained-vit-v2.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_pytorch.vit_for_small_dataset import ViT\n",
    "# Load the model\n",
    "def load_model(model_path, device, num_classes):\n",
    "    model = ViT(\n",
    "        image_size = 224,\n",
    "        patch_size = 32,\n",
    "        num_classes = num_classes,\n",
    "        dim = 128,\n",
    "        depth = 6,\n",
    "        heads = 16,\n",
    "        mlp_dim = 2048,\n",
    "        dropout = 0.1,\n",
    "        emb_dropout = 0.1\n",
    "    ).to(device)\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    return model\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "def evaluate_model(model, test_loader, device, y_pred, y_true):\n",
    "    model.to(device)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        epoch_val_accuracy = 0\n",
    "        epoch_val_loss = 0\n",
    "        for data, label in test_loader:\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            val_output = model(data)\n",
    "            val_loss = criterion(val_output, label)\n",
    "\n",
    "            acc = (val_output.argmax(dim=1) == label).float().mean()\n",
    "            epoch_val_accuracy += acc / len(test_loader)\n",
    "            epoch_val_loss += val_loss / len(test_loader)\n",
    "\n",
    "            y_pred.append(val_output.argmax(dim=1))\n",
    "            y_true.append(label)\n",
    "\n",
    "    return acc, y_pred, y_true\n",
    "\n",
    "def main():\n",
    "    # Paths and parameters\n",
    "    model_path = './trained-vit-v2.pt'\n",
    "    num_classes = 2  # Adjust this according to your dataset\n",
    "    batch_size = 32\n",
    "    model = load_model(model_path,device, num_classes)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = evaluate_model(model, test_loader, device)\n",
    "    print(f'Accuracy of the model on the test dataset: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test dataset: 74.78%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12170\n",
      "Test Data: 12170\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "root_dir = './test'\n",
    "\n",
    "test_list = []\n",
    "for subdir, dirs, files in os.walk(root_dir):\n",
    "    if os.path.basename(subdir) == 'imgs':\n",
    "        for file in files:\n",
    "            test_list.append(os.path.join(subdir, file))\n",
    "\n",
    "print(len(test_list))\n",
    "random.shuffle(test_list)\n",
    "\n",
    "print(f\"Test Data: {len(test_list)}\")\n",
    "\n",
    "test_data = ObjectClassification(test_list, transform=test_transforms)\n",
    "\n",
    "test_loader = DataLoader(dataset = test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "model_path = './trained-vit-v2.pt'\n",
    "num_classes = 2  # Adjust this according to your dataset\n",
    "batch_size = 32\n",
    "model = load_model(model_path,device, num_classes)\n",
    "\n",
    "accuracy, y_pred, y_true= evaluate_model(model, test_loader, device, y_pred, y_true)\n",
    "\n",
    "y_true_list = [tensor.cpu().numpy().flatten() for tensor in y_true]\n",
    "yt = np.concatenate(y_true_list)\n",
    "y_pred_list = [tensor.cpu().numpy().flatten() for tensor in y_pred]\n",
    "yp = np.concatenate(y_pred_list)\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(yt, yp)\n",
    "\n",
    "cm_display = ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './trained-vit-v2.pt'\n",
    "num_classes = 2  # Adjust this according to your dataset\n",
    "batch_size = 32\n",
    "model = load_model(model_path,device, num_classes)\n",
    "\n",
    "# print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ViT' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m proba \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m(test_data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ViT' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "proba = model.predict(test_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x80 in position 64: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# image = Image.open(requests.get(url, stream=True).raw)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m feature_extractor \u001b[38;5;241m=\u001b[39m \u001b[43mViTFeatureExtractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./trained-vit-v2.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m ViTForImageClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./trained-vit-v2.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/image_processing_base.py:206\u001b[0m, in \u001b[0;36mImageProcessingMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token\n\u001b[0;32m--> 206\u001b[0m image_processor_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_processor_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_dict(image_processor_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/image_processing_base.py:364\u001b[0m, in \u001b[0;36mImageProcessingMixin.get_image_processor_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;66;03m# Load image_processor dict\u001b[39;00m\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(resolved_image_processor_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m reader:\n\u001b[0;32m--> 364\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    365\u001b[0m     image_processor_dict \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(text)\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError:\n",
      "File \u001b[0;32m/usr/lib/python3.10/codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m--> 322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m data[consumed:]\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 64: invalid start byte"
     ]
    }
   ],
   "source": [
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "# image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('./trained-vit-v2.pt')\n",
    "model = ViTForImageClassification.from_pretrained('./trained-vit-v2.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViT(\n",
       "  (to_patch_embedding): SPT(\n",
       "    (to_patch_tokens): Sequential(\n",
       "      (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=32, p2=32)\n",
       "      (1): LayerNorm((15360,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): Linear(in_features=15360, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (transformer): Transformer(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x ModuleList(\n",
       "        (0): LSA(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (attend): Softmax(dim=-1)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (to_qkv): Linear(in_features=128, out_features=3072, bias=False)\n",
       "          (to_out): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (1): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): FeedForward(\n",
       "          (net): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "            (2): GELU(approximate='none')\n",
       "            (3): Dropout(p=0.1, inplace=False)\n",
       "            (4): Linear(in_features=2048, out_features=128, bias=True)\n",
       "            (5): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (to_latent): Identity()\n",
       "  (mlp_head): Sequential(\n",
       "    (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ViT(\n",
    "        image_size = 224,\n",
    "        patch_size = 32,\n",
    "        num_classes = num_classes,\n",
    "        dim = 128,\n",
    "        depth = 6,\n",
    "        heads = 16,\n",
    "        mlp_dim = 2048,\n",
    "        dropout = 0.1,\n",
    "        emb_dropout = 0.1\n",
    "    ).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('./trained-vit-v2.pt'))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12170\n",
      "Test Data: 12170\n"
     ]
    }
   ],
   "source": [
    "root_dir = './test'\n",
    "\n",
    "test_list = []\n",
    "for subdir, dirs, files in os.walk(root_dir):\n",
    "    if os.path.basename(subdir) == 'imgs':\n",
    "        for file in files:\n",
    "            test_list.append(os.path.join(subdir, file))\n",
    "\n",
    "print(len(test_list))\n",
    "random.shuffle(test_list)\n",
    "\n",
    "print(f\"Test Data: {len(test_list)}\")\n",
    "\n",
    "test_data = ObjectClassification(test_list, transform=test_transforms)\n",
    "\n",
    "test_loader = DataLoader(dataset = test_data, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.7581e-01, 6.2419e-01],\n",
      "        [2.1767e-01, 7.8233e-01],\n",
      "        [8.4610e-01, 1.5390e-01],\n",
      "        [3.9866e-02, 9.6013e-01],\n",
      "        [8.2108e-01, 1.7892e-01],\n",
      "        [7.4258e-01, 2.5742e-01],\n",
      "        [8.6191e-01, 1.3809e-01],\n",
      "        [1.1362e-03, 9.9886e-01],\n",
      "        [8.3979e-01, 1.6021e-01],\n",
      "        [3.5616e-03, 9.9644e-01],\n",
      "        [9.5489e-01, 4.5111e-02],\n",
      "        [6.7930e-04, 9.9932e-01],\n",
      "        [1.8746e-02, 9.8125e-01],\n",
      "        [8.6039e-01, 1.3961e-01],\n",
      "        [7.7226e-01, 2.2774e-01],\n",
      "        [6.0009e-02, 9.3999e-01],\n",
      "        [2.1385e-01, 7.8615e-01],\n",
      "        [9.7871e-02, 9.0213e-01],\n",
      "        [5.1900e-04, 9.9948e-01],\n",
      "        [8.3738e-01, 1.6262e-01],\n",
      "        [8.9347e-02, 9.1065e-01],\n",
      "        [4.6369e-02, 9.5363e-01],\n",
      "        [7.1869e-04, 9.9928e-01],\n",
      "        [8.8824e-01, 1.1176e-01],\n",
      "        [7.1499e-04, 9.9929e-01],\n",
      "        [5.4766e-01, 4.5234e-01],\n",
      "        [3.4850e-03, 9.9652e-01],\n",
      "        [8.2572e-01, 1.7428e-01],\n",
      "        [1.1100e-03, 9.9889e-01],\n",
      "        [6.7508e-04, 9.9932e-01],\n",
      "        [1.0190e-01, 8.9810e-01],\n",
      "        [1.2042e-03, 9.9880e-01]], device='cuda:0')\n",
      "tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
      "        1, 0, 1, 0, 1, 1, 1, 1], device='cuda:0') tensor([0.6242, 0.7823, 0.8461, 0.9601, 0.8211, 0.7426, 0.8619, 0.9989, 0.8398,\n",
      "        0.9964, 0.9549, 0.9993, 0.9813, 0.8604, 0.7723, 0.9400, 0.7861, 0.9021,\n",
      "        0.9995, 0.8374, 0.9107, 0.9536, 0.9993, 0.8882, 0.9993, 0.5477, 0.9965,\n",
      "        0.8257, 0.9989, 0.9993, 0.8981, 0.9988], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# test_loader = test_loader[0]\n",
    "\n",
    "with torch.no_grad():    \n",
    "    for data, label in test_loader:\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        val_output = model(data)\n",
    "        # val_loss = criterion(val_output, label)\n",
    "\n",
    "        acc = (val_output.argmax(dim=1) == label).float().mean()\n",
    "        # epoch_val_accuracy += acc / len(test_loader)\n",
    "        # epoch_val_loss += val_loss / len(test_loader)\n",
    "\n",
    "        # y_pred.append(val_output.argmax(dim=1))\n",
    "        # y_true.append(label)\n",
    "\n",
    "        # Assuming the output is logits (raw scores), apply softmax to get probabilities\n",
    "        probabilities = torch.nn.functional.softmax(val_output, dim=1)\n",
    "        print(probabilities)\n",
    "\n",
    "# Get the predicted class and confidence score\n",
    "        confidence_scores, predicted_classes = torch.max(probabilities, dim=1)\n",
    "        print(predicted_classes, confidence_scores)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6242, 0.7823, 0.8461, 0.9601, 0.8211, 0.7426, 0.8619, 0.9989, 0.8398,\n",
       "        0.9964, 0.9549, 0.9993, 0.9813, 0.8604, 0.7723, 0.9400, 0.7861, 0.9021,\n",
       "        0.9995, 0.8374, 0.9107, 0.9536, 0.9993, 0.8882, 0.9993, 0.5477, 0.9965,\n",
       "        0.8257, 0.9989, 0.9993, 0.8981, 0.9988], device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confidence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "        1, 0, 1, 0, 1, 1, 1, 1], device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isacs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
